{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from gym import spaces\n",
        "\n",
        "\n",
        "class CustomFrozenLake(gym.Env):\n",
        "    def __init__(self, nrow=8, ncol=8, num_holes=8):\n",
        "        self.nrow = nrow\n",
        "        self.ncol = ncol\n",
        "        self.n_state = nrow * ncol\n",
        "        self.num_holes = num_holes\n",
        "\n",
        "        # Initialize the grid with no holes\n",
        "        self.desc = np.full((nrow, ncol), b'F', dtype='c')\n",
        "\n",
        "        # Randomly place holes on the grid\n",
        "        hole_positions = np.random.choice(self.n_state, size=num_holes, replace=False)\n",
        "        for pos in hole_positions:\n",
        "            row, col = divmod(pos, ncol)\n",
        "            self.desc[row, col] = b'H'\n",
        "\n",
        "        # Start from the top-left corner\n",
        "        self.s = 0\n",
        "\n",
        "        # Define actions (Left, Down, Right, Up)\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "\n",
        "        # Define states\n",
        "        self.observation_space = spaces.Discrete(self.n_state)\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset to the starting state\n",
        "        self.s = 0\n",
        "        return self.s\n",
        "\n",
        "    def step(self, a):\n",
        "        i, j = divmod(self.s, self.ncol)\n",
        "\n",
        "        if a == 0:  # Left\n",
        "            j = max(0, j - 1)\n",
        "        elif a == 1:  # Down\n",
        "            i = min(self.nrow - 1, i + 1)\n",
        "        elif a == 2:  # Right\n",
        "            j = min(self.ncol - 1, j + 1)\n",
        "        elif a == 3:  # Up\n",
        "            i = max(0, i - 1)\n",
        "\n",
        "        new_s = i * self.ncol + j\n",
        "        self.s = new_s\n",
        "\n",
        "        # Define rewards and termination conditions\n",
        "        if self.desc[i, j] == b'G':\n",
        "            return new_s, 1, True, {}\n",
        "        elif self.desc[i, j] == b'H':\n",
        "            return new_s, -1, True, {}\n",
        "        else:\n",
        "            return new_s, 0, False, {}\n",
        "\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, env, learning_rate=0.1, discount_factor=0.99, epsilon=1.0, epsilon_decay=0.999, epsilon_min=0.01):\n",
        "        self.env = env\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.action_space = env.action_space.n\n",
        "        self.state_space = env.observation_space.n\n",
        "        self.q_table = np.zeros((self.state_space, self.action_space))\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return self.env.action_space.sample()  # Exploration: choose random action\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state, :])  # Exploitation: choose action with max Q-value\n",
        "\n",
        "    def update_q_table(self, state, action, reward, new_state):\n",
        "        max_next_action = np.max(self.q_table[new_state, :])\n",
        "        self.q_table[state, action] = (1 - self.learning_rate) * self.q_table[state, action] + \\\n",
        "                                      self.learning_rate * (reward + self.discount_factor * max_next_action)\n",
        "\n",
        "    def train(self, num_episodes=1000, max_steps_per_episode=100):\n",
        "        rewards = []\n",
        "        paths = []\n",
        "        for episode in range(num_episodes):\n",
        "            state = self.env.reset()\n",
        "            total_reward = 0\n",
        "            path = [state]\n",
        "            for step in range(max_steps_per_episode):\n",
        "                action = self.choose_action(state)\n",
        "                new_state, reward, done, _ = self.env.step(action)\n",
        "                self.update_q_table(state, action, reward, new_state)\n",
        "                total_reward += reward\n",
        "                #\n",
        "                path.append(new_state)\n",
        "                state = new_state\n",
        "                if done:\n",
        "                    break\n",
        "            rewards.append(total_reward)\n",
        "            #\n",
        "            paths.append(path)\n",
        "            # Decay epsilon for exploration-exploitation trade-off\n",
        "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "        return rewards, paths\n",
        "\n",
        "# Create the custom FrozenLake environment\n",
        "env = CustomFrozenLake()\n",
        "\n",
        "# Create Q-learning agent and train it in the environment\n",
        "agent = QLearningAgent(env)\n",
        "episodes = 1000\n",
        "rewards_per_episode = agent.train(num_episodes=episodes)\n",
        "#\n",
        "rewards_per_episode, paths = agent.train(num_episodes=episodes)\n",
        "# Print average rewards per episode\n",
        "print(f\"Average rewards over {episodes} episodes: {np.mean(rewards_per_episode)}\")\n",
        "\n",
        "# Print paths for the first few episodes\n",
        "for i in range(min(5, episodes)):\n",
        "    print(f\"Episode {i + 1} Path: {paths[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qUznusO3n5p",
        "outputId": "97707db9-997c-4633-9dd8-288e3824f27e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average rewards over 1000 episodes: -0.854\n",
            "Episode 1 Path: [0, 0, 0, 0, 0, 0, 0, 1, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 8, 8, 16]\n",
            "Episode 2 Path: [0, 0, 0, 8, 8, 16]\n",
            "Episode 3 Path: [0, 0, 0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 8, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 16]\n",
            "Episode 4 Path: [0, 0, 8, 8, 8, 8, 8, 9, 8, 8, 8, 8, 8, 8, 16]\n",
            "Episode 5 Path: [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 8, 0, 1, 0, 0, 0, 8, 8, 8, 9, 8, 16]\n"
          ]
        }
      ]
    }
  ]
}