{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "class CustomFrozenLakeEnv(gym.Env):\n",
        "    # (unchanged from previous code)\n",
        "    def __init__(self, size=16, num_holes=10, num_bonus=10):\n",
        "        self.size = size\n",
        "        self.num_holes = num_holes\n",
        "        self.num_bonus = num_bonus\n",
        "        self.grid = np.zeros((size, size), dtype=np.int8)\n",
        "        self.start_pos = (0, 0)\n",
        "        self.goal_pos = (size - 1, size - 1)\n",
        "\n",
        "        hole_positions = [(np.random.randint(size), np.random.randint(size)) for _ in range(num_holes)]\n",
        "        for pos in hole_positions:\n",
        "            self.grid[pos] = 1  # Mark as hole\n",
        "\n",
        "        bonus_positions = [(np.random.randint(size), np.random.randint(size)) for _ in range(num_bonus)]\n",
        "        for pos in bonus_positions:\n",
        "            self.grid[pos] = 10  # Mark as bonus reward\n",
        "\n",
        "        self.action_space = spaces.Discrete(4)  # Up, Down, Left, Right\n",
        "        self.observation_space = spaces.Discrete(size * size)\n",
        "\n",
        "        self.agent_pos = self.start_pos\n",
        "        self.done = False\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_pos = self.start_pos\n",
        "        self.done = False\n",
        "        return self.get_state()\n",
        "\n",
        "    def get_state(self):\n",
        "        return self.agent_pos[0] * self.size + self.agent_pos[1]\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.done:\n",
        "            raise ValueError(\"Episode is done. Call reset() to start a new episode.\")\n",
        "\n",
        "        row, col = self.agent_pos\n",
        "\n",
        "        if action == 0:  # Up\n",
        "            row = max(0, row - 1)\n",
        "        elif action == 1:  # Down\n",
        "            row = min(self.size - 1, row + 1)\n",
        "        elif action == 2:  # Left\n",
        "            col = max(0, col - 1)\n",
        "        elif action == 3:  # Right\n",
        "            col = min(self.size - 1, col + 1)\n",
        "\n",
        "        self.agent_pos = (row, col)\n",
        "\n",
        "        if self.grid[self.agent_pos] == 1:  # Agent fell into a hole\n",
        "            reward = -1\n",
        "            self.done = True\n",
        "        elif self.grid[self.agent_pos] == 10:  # Agent found a bonus reward\n",
        "            reward = 10\n",
        "            self.grid[self.agent_pos] = 0  # Remove bonus reward from the grid\n",
        "        elif self.agent_pos == self.goal_pos:  # Agent reached the goal\n",
        "            reward = 1\n",
        "            self.done = True\n",
        "        else:\n",
        "            reward = 0\n",
        "\n",
        "        return self.get_state(), reward, self.done, {}\n",
        "\n",
        "    def render(self):\n",
        "        render_grid = np.zeros_like(self.grid, dtype=str)\n",
        "        render_grid[self.grid == 1] = 'H'\n",
        "        render_grid[self.grid == 10] = '+'\n",
        "        render_grid[self.start_pos] = 'S'\n",
        "        render_grid[self.goal_pos] = 'G'\n",
        "        render_grid[self.agent_pos] = 'A'\n",
        "\n",
        "        print(render_grid)\n",
        "\n",
        "class QNetwork(tf.keras.Model):\n",
        "    def __init__(self, num_actions):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.layer1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.layer2 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.output_layer = tf.keras.layers.Dense(num_actions, activation='linear')\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.layer1(state)\n",
        "        x = self.layer2(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "num_actions = 4\n",
        "q_network = QNetwork(num_actions)\n",
        "\n",
        "learning_rate = 0.001\n",
        "gamma = 0.99\n",
        "optimizer = tf.optimizers.Adam(learning_rate)\n",
        "huber_loss = tf.keras.losses.Huber()\n",
        "\n",
        "num_episodes = 1000\n",
        "epsilon = 0.1\n",
        "\n",
        "env = CustomFrozenLakeEnv(size=16, num_holes=10, num_bonus=10)\n",
        "\n",
        "max_reward = float('-inf')\n",
        "best_path = None\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, 1]).astype(np.float32)\n",
        "\n",
        "    total_reward = 0\n",
        "    episode_path = [env.agent_pos]  # Store the initial position\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        for t in range(1000):  # Maximum number of steps per episode\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = np.random.randint(env.action_space.n)\n",
        "            else:\n",
        "                q_values = q_network(state)\n",
        "                action = tf.argmax(q_values, axis=1).numpy()[0]\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_state = np.reshape(next_state, [1, 1]).astype(np.float32)\n",
        "\n",
        "            episode_path.append(env.agent_pos)\n",
        "\n",
        "            q_values_next = q_network(next_state)\n",
        "            target = reward + gamma * tf.reduce_max(q_values_next, axis=1)\n",
        "            loss = huber_loss(target, q_network(state))\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "    grads = tape.gradient(loss, q_network.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, q_network.trainable_variables))\n",
        "\n",
        "    print(\"Episode {}: Total Reward: {}\".format(episode, total_reward))\n",
        "\n",
        "    if total_reward > max_reward:\n",
        "        max_reward = total_reward\n",
        "        best_path = episode_path.copy()\n",
        "\n",
        "    if done:\n",
        "        if reward == 1:\n",
        "            print(\"Game completed successfully! Agent reached the goal.\")\n",
        "        else:\n",
        "            print(\"Agent fell into a hole. Game over.\")\n",
        "            print(\"Path Traversed:\", episode_path)\n",
        "            print(\"Maximum Reward Achieved:\", max_reward)\n",
        "            print(\"Episodes Trained For:\", episode + 1)\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRGYnqZgrf-o",
        "outputId": "3cfa0f27-d6ba-4b7d-f287-4d9349ee1d66"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Total Reward: 20\n",
            "Episode 1: Total Reward: -1\n",
            "Agent fell into a hole. Game over.\n",
            "Path Traversed: [(0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 1), (1, 0), (1, 1), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 1), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (2, 0), (3, 0), (3, 0), (3, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 1), (4, 0), (4, 0), (4, 0), (4, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (4, 0), (4, 0), (4, 0), (4, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (4, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (3, 0), (3, 0), (3, 0), (3, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 1), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 1), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 1), (1, 0), (2, 0), (2, 0), (2, 0), (2, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 1), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 1), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 1), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (3, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 1), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (4, 0), (5, 0), (5, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (7, 0)]\n",
            "Maximum Reward Achieved: 20\n",
            "Episodes Trained For: 2\n"
          ]
        }
      ]
    }
  ]
}